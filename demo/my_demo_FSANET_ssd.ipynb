{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from math import cos, sin\n",
    "# from moviepy.editor import *\n",
    "from lib.FSANET_model import *\n",
    "import numpy as np\n",
    "from keras.layers import Average\n",
    "# from moviepy.editor import *\n",
    "# from mtcnn.mtcnn import MTCNN\n",
    "from time import perf_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def draw_axis(img_idx, img, yaw, pitch, roll, tdx=None, tdy=None, size = 50):\n",
    "    txt_out.write(str(img_idx) + ' %f %f %f\\n' % (yaw, pitch, roll))\n",
    "    pitch = pitch * np.pi / 180\n",
    "    yaw = -(yaw * np.pi / 180)\n",
    "    roll = roll * np.pi / 180\n",
    "\n",
    "    if tdx != None and tdy != None:\n",
    "        tdx = tdx\n",
    "        tdy = tdy\n",
    "    else:\n",
    "        height, width = img.shape[:2]\n",
    "        tdx = width / 2\n",
    "        tdy = height / 2\n",
    "\n",
    "    # X-Axis pointing to right. drawn in red\n",
    "    x1 = size * (cos(yaw) * cos(roll)) + tdx\n",
    "    y1 = size * (cos(pitch) * sin(roll) + cos(roll) * sin(pitch) * sin(yaw)) + tdy\n",
    "\n",
    "    # Y-Axis | drawn in green\n",
    "    #        v\n",
    "    x2 = size * (-cos(yaw) * sin(roll)) + tdx\n",
    "    y2 = size * (cos(pitch) * cos(roll) - sin(pitch) * sin(yaw) * sin(roll)) + tdy\n",
    "\n",
    "    # Z-Axis (out of the screen) drawn in blue\n",
    "    x3 = size * (sin(yaw)) + tdx\n",
    "    y3 = size * (-cos(yaw) * sin(pitch)) + tdy\n",
    "\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x1),int(y1)),(0,0,255),3)\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x2),int(y2)),(0,255,0),3)\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x3),int(y3)),(255,0,0),2)\n",
    "\n",
    "    return img\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def draw_results_ssd(img_idx, detected,input_img,faces,ad,img_size,img_w,img_h,model,time_detection,time_network,time_plot):\n",
    "\n",
    "    # loop over the detections\n",
    "    if detected.shape[2]>0:\n",
    "        for i in range(0, detected.shape[2]):\n",
    "            # extract the confidence (i.e., probability) associated with the\n",
    "            # prediction\n",
    "            confidence = detected[0, 0, i, 2]\n",
    "\n",
    "            # filter out weak detections\n",
    "            if confidence > 0.5:\n",
    "                # compute the (x, y)-coordinates of the bounding box for\n",
    "                # the face and extract the face ROI\n",
    "                (h0, w0) = input_img.shape[:2]\n",
    "                box = detected[0, 0, i, 3:7] * np.array([w0, h0, w0, h0])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                # print((startX, startY, endX, endY))\n",
    "                x1 = startX\n",
    "                y1 = startY\n",
    "                w = endX - startX\n",
    "                h = endY - startY\n",
    "\n",
    "                x2 = x1+w\n",
    "                y2 = y1+h\n",
    "\n",
    "                xw1 = max(int(x1 - ad * w), 0)\n",
    "                yw1 = max(int(y1 - ad * h), 0)\n",
    "                xw2 = min(int(x2 + ad * w), img_w - 1)\n",
    "                yw2 = min(int(y2 + ad * h), img_h - 1)\n",
    "\n",
    "                faces[i,:,:,:] = cv2.resize(input_img[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "                faces[i,:,:,:] = cv2.normalize(faces[i,:,:,:], None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "                face = np.expand_dims(faces[i,:,:,:], axis=0)\n",
    "                p_result = model.predict(face)\n",
    "\n",
    "                face = face.squeeze()\n",
    "                img = draw_axis(img_idx, input_img[yw1:yw2 + 1, xw1:xw2 + 1, :], p_result[0][0], p_result[0][1], p_result[0][2])\n",
    "\n",
    "                input_img[yw1:yw2 + 1, xw1:xw2 + 1, :] = img\n",
    "\n",
    "    cv2.imshow(\"result\", input_img)\n",
    "\n",
    "    return input_img #,time_network,time_plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# custom variables\n",
    "start = perf_counter()\n",
    "video_path = \"../../data/testvid_small.mp4\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# try:\n",
    "#     os.mkdir('./img')\n",
    "# except OSError:\n",
    "#     print(\"Directory ./img already exists.\")\n",
    "#     pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# load model and weights\n",
    "img_size = 64\n",
    "stage_num = [3,3,3]\n",
    "lambda_local = 1\n",
    "lambda_d = 1\n",
    "img_idx = 0\n",
    "detected = '' #make this not local variable\n",
    "time_detection = 0\n",
    "time_network = 0\n",
    "time_plot = 0\n",
    "skip_frame = 1 # original: 5; every 5 frame do 1 detection and network forward propagation\n",
    "ad = 0.6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d_7/Sigmoid:0\", shape=(None, 8, 8, 1), dtype=float32)\n",
      "Tensor(\"reshape_1/Reshape:0\", shape=(None, None), dtype=float32)\n",
      "Tensor(\"moments_layer_1/moments/Squeeze_1:0\", shape=(None, 8, 8), dtype=float32)\n",
      "Tensor(\"reshape_13/Reshape:0\", shape=(None, None), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "num_capsule = 3\n",
    "dim_capsule = 16\n",
    "routings = 2\n",
    "stage_num = [3,3,3]\n",
    "lambda_d = 1\n",
    "num_classes = 3\n",
    "image_size = 64\n",
    "num_primcaps = 7*3\n",
    "m_dim = 5\n",
    "S_set = [num_capsule, dim_capsule, routings, num_primcaps, m_dim]\n",
    "\n",
    "model1 = FSA_net_Capsule(image_size, num_classes, stage_num, lambda_d, S_set)()\n",
    "model2 = FSA_net_Var_Capsule(image_size, num_classes, stage_num, lambda_d, S_set)()\n",
    "\n",
    "num_primcaps = 8*8*3\n",
    "S_set = [num_capsule, dim_capsule, routings, num_primcaps, m_dim]\n",
    "\n",
    "model3 = FSA_net_noS_Capsule(image_size, num_classes, stage_num, lambda_d, S_set)()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models ...\n",
      "Finished loading model 1.\n",
      "Finished loading model 2.\n",
      "Finished loading model 3.\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "print('Loading models ...')\n",
    "\n",
    "weight_file1 = '../pre-trained/300W_LP_models/fsanet_capsule_3_16_2_21_5/fsanet_capsule_3_16_2_21_5.h5'\n",
    "model1.load_weights(weight_file1)\n",
    "print('Finished loading model 1.')\n",
    "\n",
    "weight_file2 = '../pre-trained/300W_LP_models/fsanet_var_capsule_3_16_2_21_5/fsanet_var_capsule_3_16_2_21_5.h5'\n",
    "model2.load_weights(weight_file2)\n",
    "print('Finished loading model 2.')\n",
    "\n",
    "weight_file3 = '../pre-trained/300W_LP_models/fsanet_noS_capsule_3_16_2_192_5/fsanet_noS_capsule_3_16_2_192_5.h5'\n",
    "model3.load_weights(weight_file3)\n",
    "print('Finished loading model 3.')\n",
    "\n",
    "inputs = Input(shape=(64,64,3))\n",
    "x1 = model1(inputs) #1x1\n",
    "x2 = model2(inputs) #var\n",
    "x3 = model3(inputs) #w/o\n",
    "avg_model = Average()([x1,x2,x3])\n",
    "model = Model(inputs=inputs, outputs=avg_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector...\n"
     ]
    }
   ],
   "source": [
    "# load our serialized face detector from disk\n",
    "print(\"[INFO] loading face detector...\")\n",
    "protoPath = os.path.sep.join([\"face_detector\", \"deploy.prototxt\"])\n",
    "modelPath = os.path.sep.join([\"face_detector\",\n",
    "    \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# capture video\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1024*1)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 768*1)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1024*1)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 768*1)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # float\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # float"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out1 = cv2.VideoWriter('output/video/output-FSANET_ssd1.avi' , fourcc, 1, (width, height))\n",
    "out30 = cv2.VideoWriter('output/video/output-FSANET_ssd30.avi', fourcc, 30, (width, height))\n",
    "txt_out = open('output/video/output-FSANET_ssd.txt', 'w')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start detecting pose ...\n"
     ]
    }
   ],
   "source": [
    "# Pose detection\n",
    "print('Start detecting pose ...')\n",
    "detected_pre = np.empty((1,1,1))\n",
    "\n",
    "while True:\n",
    "    # get video frame\n",
    "    ret, input_img = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "    img_idx = img_idx + 1\n",
    "    img_h, img_w, _ = np.shape(input_img)\n",
    "\n",
    "    if img_idx==1 or img_idx%skip_frame == 0:\n",
    "        time_detection = 0\n",
    "        time_network = 0\n",
    "        time_plot = 0\n",
    "\n",
    "        # detect faces using LBP detector\n",
    "        gray_img = cv2.cvtColor(input_img,cv2.COLOR_BGR2GRAY)\n",
    "        # detected = face_cascade.detectMultiScale(gray_img, 1.1)\n",
    "        # detected = detector.detect_faces(input_img)\n",
    "        # pass the blob through the network and obtain the detections and\n",
    "        # predictions\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(input_img, (300, 300)), 1.0,\n",
    "            (300, 300), (104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detected = net.forward()\n",
    "\n",
    "        if detected_pre.shape[2] > 0 and detected.shape[2] == 0:\n",
    "            detected = detected_pre\n",
    "\n",
    "        faces = np.empty((detected.shape[2], img_size, img_size, 3))\n",
    "\n",
    "        input_img = draw_results_ssd(img_idx, detected,input_img,faces,ad,img_size,img_w,img_h,model,time_detection,time_network,time_plot)\n",
    "        # cv2.imwrite('img/'+str(img_idx)+'.png',input_img)\n",
    "        out1.write(input_img)\n",
    "        out30.write(input_img)\n",
    "\n",
    "    else:\n",
    "        input_img = draw_results_ssd(img_idx, detected,input_img,faces,ad,img_size,img_w,img_h,model,time_detection,time_network,time_plot)\n",
    "\n",
    "\n",
    "    if detected.shape[2] > detected_pre.shape[2] or img_idx%(skip_frame*3) == 0:\n",
    "        detected_pre = detected\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "txt_out.write(\"Runtime: \" + str(perf_counter() - start) + \"seconds\")\n",
    "\n",
    "out1.release()\n",
    "out30.release()\n",
    "cap.release()\n",
    "txt_out.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-3a31f57d",
   "language": "python",
   "display_name": "PyCharm (Master)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}